{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# 환경 변수 설정\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama LLM 설정\n",
    "llm = Ollama(\n",
    "    model=\"deepseek-r1:8b\",  # Ollama에서 제공하는 deepseek-r1:8b 모델 사용\n",
    "    temperature=0.1,  # 응답의 창의성 조절 (0에 가까울수록 일관된 응답)\n",
    "    context_window=4096,  # 컨텍스트 윈도우 크기\n",
    "    timeout=120,  # 타임아웃 시간을 120초로 설정\n",
    "    request_timeout=120,  # 요청 타임아웃도 120초로 설정\n",
    "    additional_kwargs={  # 추가 파라미터\n",
    "        \"num_gpu\": 0,  # CPU 사용 설정\n",
    "        \"seed\": 42,  # 재현성을 위한 시드 설정\n",
    "    },\n",
    ")\n",
    "\n",
    "# BGE 임베딩 모델 설정\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-m3\", device=\"cpu\"  # CPU 사용. GPU의 경우 \"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ServiceContext 설정\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# PDF 문서 로드\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"../data/pdf_sample1/240828_(AI리포트)_미국의_인공지능(AI)_정책,전략.pdf\"],\n",
    "    filename_as_id=True).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스 생성\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# 쿼리 엔진 생성\n",
    "query_engine = index.as_query_engine(\n",
    "    streaming=True,  # 스트리밍 응답 활성화\n",
    "    similarity_top_k=3,  # 상위 3개의 가장 관련성 높은 문서 청크 사용\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문하기\n",
    "response = query_engine.query(\n",
    "    \"\"\"\n",
    "    책임 있는 AI 개발에서 미국의 리더십을 명확히 하는 데에\n",
    "    AI안전 연구소가 맡은 중요한 역할이 있다고 생각한다고 말한 사람과 \n",
    "    그 사람이 누구와 협력한다고 했는지 알려줘\n",
    "    \"\"\"\n",
    ")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LlamaIndex_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
