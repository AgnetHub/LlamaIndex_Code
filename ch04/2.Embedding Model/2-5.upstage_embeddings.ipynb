{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-embeddings-upstage==0.2.1\n",
      "  Using cached llama_index_embeddings_upstage-0.2.1-py3-none-any.whl (5.0 kB)\n",
      "Collecting llama-index-core<0.12.0,>=0.11.0\n",
      "  Downloading llama_index_core-0.11.23-py3-none-any.whl (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 508.9 kB/s eta 0:00:00\n",
      "Collecting llama-index-embeddings-openai<0.3.0,>=0.2.0\n",
      "  Downloading llama_index_embeddings_openai-0.2.5-py3-none-any.whl (6.1 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (2025.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (1.26.4)\n",
      "Requirement already satisfied: wrapt in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (1.17.2)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (1.0.8)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (3.4.2)\n",
      "Requirement already satisfied: dataclasses-json in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (0.6.7)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (4.12.2)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (1.2.18)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (1.2.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (1.6.0)\n",
      "Requirement already satisfied: httpx in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (0.28.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (3.9.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (10.4.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.0 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (2.10.6)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (3.11.12)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (2.0.38)\n",
      "Requirement already satisfied: openai>=1.1.0 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from llama-index-embeddings-openai<0.3.0,>=0.2.0->llama-index-embeddings-upstage==0.2.1) (1.64.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (1.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (2.4.6)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (1.18.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (5.0.1)\n",
      "Requirement already satisfied: joblib in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (1.4.2)\n",
      "Requirement already satisfied: click in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (2024.11.6)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.3.0,>=0.2.0->llama-index-embeddings-upstage==0.2.1) (1.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.3.0,>=0.2.0->llama-index-embeddings-upstage==0.2.1) (4.8.0)\n",
      "Requirement already satisfied: sniffio in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.3.0,>=0.2.0->llama-index-embeddings-upstage==0.2.1) (1.3.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.3.0,>=0.2.0->llama-index-embeddings-upstage==0.2.1) (0.8.2)\n",
      "Requirement already satisfied: certifi in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (2025.1.31)\n",
      "Requirement already satisfied: idna in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (3.10)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (0.14.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (2.27.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (2.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (3.26.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index-embeddings-openai<0.3.0,>=0.2.0->llama-index-embeddings-upstage==0.2.1) (1.2.2)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\code\\llamaindex_code-main\\llamaindex_venv\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-embeddings-upstage==0.2.1) (24.2)\n",
      "Installing collected packages: tenacity, llama-index-core, llama-index-embeddings-openai, llama-index-embeddings-upstage\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 9.0.0\n",
      "    Uninstalling tenacity-9.0.0:\n",
      "      Successfully uninstalled tenacity-9.0.0\n",
      "  Attempting uninstall: llama-index-core\n",
      "    Found existing installation: llama-index-core 0.12.27\n",
      "    Uninstalling llama-index-core-0.12.27:\n",
      "      Successfully uninstalled llama-index-core-0.12.27\n",
      "  Attempting uninstall: llama-index-embeddings-openai\n",
      "    Found existing installation: llama-index-embeddings-openai 0.3.1\n",
      "    Uninstalling llama-index-embeddings-openai-0.3.1:\n",
      "      Successfully uninstalled llama-index-embeddings-openai-0.3.1\n",
      "Successfully installed llama-index-core-0.11.23 llama-index-embeddings-openai-0.2.5 llama-index-embeddings-upstage-0.2.1 tenacity-8.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index 0.12.27 requires llama-index-core<0.13.0,>=0.12.27, but you have llama-index-core 0.11.23 which is incompatible.\n",
      "llama-index 0.12.27 requires llama-index-embeddings-openai<0.4.0,>=0.3.0, but you have llama-index-embeddings-openai 0.2.5 which is incompatible.\n",
      "llama-index-vector-stores-qdrant 0.4.3 requires llama-index-core<0.13.0,>=0.12.7, but you have llama-index-core 0.11.23 which is incompatible.\n",
      "llama-index-vector-stores-faiss 0.3.0 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.11.23 which is incompatible.\n",
      "llama-index-vector-stores-chroma 0.4.1 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.11.23 which is incompatible.\n",
      "llama-index-retrievers-bm25 0.5.2 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.11.23 which is incompatible.\n",
      "llama-index-readers-llama-parse 0.4.0 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.11.23 which is incompatible.\n",
      "llama-index-readers-file 0.4.5 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.11.23 which is incompatible.\n",
      "llama-index-question-gen-openai 0.3.0 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.11.23 which is incompatible.\n",
      "llama-index-program-openai 0.3.1 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.11.23 which is incompatible.\n",
      "llama-index-multi-modal-llms-openai 0.4.3 requires llama-index-core<0.13.0,>=0.12.3, but you have llama-index-core 0.11.23 which is incompatible.\n",
      "llama-index-llms-openai 0.3.20 requires llama-index-core<0.13.0,>=0.12.17, but you have llama-index-core 0.11.23 which is incompatible.\n",
      "llama-index-llms-ollama 0.5.4 requires llama-index-core<0.13.0,>=0.12.4, but you have llama-index-core 0.11.23 which is incompatible.\n",
      "llama-index-llms-huggingface 0.4.2 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.11.23 which is incompatible.\n",
      "llama-index-llms-gemini 0.4.14 requires llama-index-core<0.13.0,>=0.12.12, but you have llama-index-core 0.11.23 which is incompatible.\n",
      "llama-index-llms-anthropic 0.6.10 requires llama-index-core<0.13.0,>=0.12.5, but you have llama-index-core 0.11.23 which is incompatible.\n",
      "llama-index-indices-managed-llama-cloud 0.6.8 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.11.23 which is incompatible.\n",
      "llama-index-embeddings-huggingface 0.5.1 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.11.23 which is incompatible.\n",
      "llama-index-embeddings-gemini 0.3.2 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.11.23 which is incompatible.\n",
      "llama-index-embeddings-cohere 0.4.0 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.11.23 which is incompatible.\n",
      "llama-index-embeddings-clip 0.4.0 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.11.23 which is incompatible.\n",
      "llama-index-cli 0.4.1 requires llama-index-core<0.13.0,>=0.12.0, but you have llama-index-core 0.11.23 which is incompatible.\n",
      "llama-index-cli 0.4.1 requires llama-index-embeddings-openai<0.4.0,>=0.3.0, but you have llama-index-embeddings-openai 0.2.5 which is incompatible.\n",
      "llama-index-agent-openai 0.4.6 requires llama-index-core<0.13.0,>=0.12.18, but you have llama-index-core 0.11.23 which is incompatible.\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-embeddings-upstage==0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# dotenv를 사용하여 환경 변수 로드\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.upstage import UpstageEmbedding\n",
    "\n",
    "embed_model = UpstageEmbedding(model='solar-embedding-1-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of embeddings: 4096\n",
      "[-0.01983642578125, -0.02679443359375, -0.02471923828125, 0.006320953369140625, -0.015716552734375]\n"
     ]
    }
   ],
   "source": [
    "# 텍스트를 벡터로 변환\n",
    "embeddings = embed_model.get_text_embedding(\"Upstage new Embeddings models is great.\")\n",
    "\n",
    "# 생성된 임베딩의 차원을 출력\n",
    "print(f\"Dimension of embeddings: {len(embeddings)}\")\n",
    "# 임베딩 벡터의 처음 5개 값을 출력\n",
    "print(embeddings[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LlamaIndex_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
