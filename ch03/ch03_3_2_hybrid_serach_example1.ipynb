{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.2 Hybrid Search Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step1. 문서 준비\n",
    "sample_documents = [\n",
    "    \"RAG 기술의 최신 동향\",\n",
    "    \"최신 검색 증강 시스템 연구\",\n",
    "    \"RAG을 활용한 챗봇 개발\",\n",
    "    \"RAG와 Dense Passage Retrieval의 비교 연구\",\n",
    "    \"최신 AI 논문: 검색 증강 기술\",\n",
    "]\n",
    "\n",
    "### Document 객체 생성\n",
    "from llama_index.core import Document\n",
    "documents = [Document(text=doc) for doc in sample_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step2. 희소 검색을 위한형태소 분석기 함수 작성\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt() # Okt 형태소 분석기 초기화\n",
    "def tokenize_korean_text(text): # 문서 내용 토큰화 함수 정의\n",
    "    return okt.morphs(text)  # 한국어 형태소 기반 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step3.LLM 및 임베딩 모델 설정\n",
    "\n",
    "## API KEY 설정\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "## 모델 설정\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "Settings.llm = OpenAI(model=\"gpt-4o\", temperature=0.5)  # 모델명은 예시\n",
    "embedding_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step4. 청킹을 위한 Splitter 설정\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\OneDrive\\Desktop\\Project\\llamaindex_practice\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resource module not available on Windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 5/5 [00:00<00:00, 1556.91it/s]\n",
      "Extracting keywords from nodes: 100%|██████████| 5/5 [00:09<00:00,  1.99s/it]\n"
     ]
    }
   ],
   "source": [
    "### Step5. Keyword(BM25) Index 설정\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.indices.keyword_table import KeywordTableIndex\n",
    "keyword_index = KeywordTableIndex.from_documents(\n",
    "    documents=documents,\n",
    "    text_splitter=splitter,\n",
    "    extract_keyword=tokenize_korean_text,\n",
    "    show_progress=True,\n",
    ")\n",
    "## bm25 retriever 객체 생성(keyword_index 연결)\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    index=keyword_index,\n",
    "    similarity_top_k=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step6. 임베딩 모델을 사용 한 VectorStoreIndex 생성\n",
    "from llama_index.core import VectorStoreIndex\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    "    text_splitter=splitter,\n",
    "    embed_model=embedding_model\n",
    ")\n",
    "# dense retriever 객체 생성(vector_index 연결)\n",
    "semantic_retriever = vector_index.as_retriever(similarity_top_k=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step7. 인덱스 db 구축(필요에 따라 구현)\n",
    "vector_index.storage_context.persist(persist_dir=\"./index/ch03_hybrid_search_vertor_storage\") # 로컬 저장\n",
    "keyword_index.storage_context.persist(persist_dir=\"./index/ch03_hybrid_search_keyword_storage\") # 로컬 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step8. 혼합 검색 함수 구현\n",
    "def hybrid_search(query, bm25_retriever, semantic_retriever, \n",
    "                bm25_weight=0.5, semantic_weight=0.5):\n",
    "    # BM25 검색 결과\n",
    "    bm25_results = bm25_retriever.retrieve(query)\n",
    "    bm25_scores = {node.text: node.score * bm25_weight for node in bm25_results}\n",
    "    \n",
    "    # Semantic 검색 결과\n",
    "    semantic_results = semantic_retriever.retrieve(query)\n",
    "    semantic_scores = {node.text: node.score * semantic_weight for node in semantic_results}\n",
    "\n",
    "    # Hybrid Score 결합\n",
    "    combined_scores = bm25_scores.copy()\n",
    "    for node_text, score in semantic_scores.items():\n",
    "        if node_text in combined_scores:\n",
    "            combined_scores[node_text] += score\n",
    "        else:\n",
    "            combined_scores[node_text] = score\n",
    "\n",
    "    # 스코어 정렬\n",
    "    sorted_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hybrid Search Results]\n",
      "1. 최신 검색 증강 시스템 연구 (score: 0.8014721855825501)\n",
      "2. 최신 AI 논문: 검색 증강 기술 (score: 0.7702755156728551)\n"
     ]
    }
   ],
   "source": [
    "### Step9. 쿼리 검색 수행\n",
    "query = \"검색 증강 생성 기술에 대해 알려주세요\"\n",
    "search_results = hybrid_search(query, bm25_retriever, semantic_retriever, bm25_weight=0.5, semantic_weight=0.5)\n",
    "\n",
    "print(\"[Hybrid Search Results]\")\n",
    "for rank, (node, score) in enumerate(search_results, start=1):\n",
    "    print(f\"{rank}. {node} (score: {score})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
