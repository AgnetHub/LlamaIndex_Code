{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step1-1. 저장 된 인덱스 불러오기\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./index/ch03_vector_index_storage\") \n",
    "vector_index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Step1-2. LLM 및 Embedding Model 설정\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "Settings.llm = OpenAI(model=\"gpt-4o\", temperature=0)  # 모델명은 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###STep2. 다중 쿼리 커스텀 클래스 생성\n",
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.indices.query.query_transform.base import DecomposeQueryTransform\n",
    "from typing import Optional, Dict, List\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.llms import LLM\n",
    "\n",
    "class MultiQueryTransform(DecomposeQueryTransform):\n",
    "    \"\"\"\n",
    "    MultiQueryTransform\n",
    "    `DecomposeQueryTransform`를 상속하여, 하나의 쿼리를 여러 개의 서브 쿼리로 분해하는 기능을 제공합니다.\n",
    "    `sub_query_num`이 몇 개인지에 따라 LLM 프롬프트를 통해 여러 개의 쿼리를 생성합니다.\n",
    "    생성된 서브 쿼리는 최종적으로 QueryBundle의 `custom_embedding_strs`에 추가됩니다.\n",
    "\n",
    "    Args:\n",
    "        sub_query_num (int): 분해될 쿼리 수\n",
    "        llm (Optional[LLM]): LLM 객체\n",
    "        verbose (bool): 중간 변환 과정을 출력할지 여부\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sub_query_num: int,\n",
    "        llm: Optional[LLM] = None,\n",
    "        verbose: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(llm=llm, verbose=verbose)\n",
    "        self.sub_query_num = sub_query_num\n",
    "\n",
    "        # 멀티 쿼리 프롬프트 정의\n",
    "        # {query_str}와 {sub_query_num}은 LLM에 전달될 때 실제 값으로 치환됩니다.\n",
    "        self._multi_query_prompt: PromptTemplate = PromptTemplate(\n",
    "            template=(\n",
    "                \"다음 [Original Query]와 관련 있는 쿼리를 {sub_query_num}가지로 변환시켜주세요.\\n\"\n",
    "                \"출력 포맷은 아래와 같은 형태를 지켜주세요. 변환 쿼리 앞에 #을 붙여주세요.\\n\\n\"\n",
    "                \"Original Query: {query_str}\\n\"\n",
    "                \"Sub-Queries:\\n\"\n",
    "                \"#변환된 쿼리\\n\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:\n",
    "        \"\"\"Run query transform.\"\"\"\n",
    "        # 기존 쿼리\n",
    "        query_str = query_bundle.query_str\n",
    "\n",
    "        # LLM에 전달하여 여러 개의 서브 쿼리를 생성\n",
    "        llm_output = self._llm.predict(\n",
    "            prompt=self._multi_query_prompt,\n",
    "            query_str=query_str,\n",
    "            sub_query_num=self.sub_query_num,\n",
    "        )\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"=== Original Query ===\\n{query_str}\\n\")\n",
    "            print(f\"=== LLM Output ===\\n{llm_output}\\n\")\n",
    "\n",
    "        # 최종적으로 QueryBundle을 생성하되,\n",
    "        # original query_str를 그대로 유지하고, sub_queries를 custom_embedding_strs에 담아 반환\n",
    "        return QueryBundle(\n",
    "            query_str=query_str,  # 원본 쿼리는 그대로 유지\n",
    "            custom_embedding_strs=self._parsing_llm_output(llm_output=llm_output),  # 분해된 쿼리를 추가\n",
    "        )\n",
    "    # 쿼리 분해 결과를 파싱하는 메서드\n",
    "    def _parsing_llm_output(self, llm_output)->List[str]:\n",
    "        # LLM으로부터 받은 결과를 파싱하여 서브 쿼리만 추출\n",
    "        # 예) Sub-Queries:\n",
    "        #     #첫번째 쿼리\n",
    "        #     #두번째 쿼리\n",
    "        sub_queries = []\n",
    "        for line in llm_output.splitlines():\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"#\"):\n",
    "                # '#' 기호를 제외한 문자열을 서브 쿼리로 간주\n",
    "                sub_query = line.lstrip(\"#\").strip()\n",
    "                if sub_query:\n",
    "                    sub_queries.append(sub_query)\n",
    "        if self.verbose:\n",
    "            print(f\"=== Parsed Sub-Queries ===\\n{sub_queries}\\n\")\n",
    "        return sub_queries\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "decompose = MultiQueryTransform(sub_query_num=2)\n",
    "query = \"기펜재가 뭐에요?\"\n",
    "query_bundle = decompose.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 쿼리 : 기펜재가 뭐에요?\n",
      "분해 쿼리 : ['기펜재의 정의는 무엇인가요?', '기펜재의 특징에 대해 설명해 주세요.']\n"
     ]
    }
   ],
   "source": [
    "print(f'원본 쿼리 : {query_bundle.query_str}')\n",
    "print(f'변환 쿼리 : {query_bundle.custom_embedding_strs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "multi_query_engine = TransformQueryEngine(query_engine=query_engine,\n",
    "                                         query_transform=decompose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = multi_query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기펜재는 일반적인 수요의 법칙과는 반대로, 가격이 하락할 때 오히려 수요량이 감소하는 재화를 말합니다. 이는 수요의 법칙에 위배되는 현상으로, 이러한 재화를 처음 관찰한 학자의 이름을 따서 기펜재라고 부릅니다. 기펜재는 열등재의 일종으로, 소득이 증가함에 따라 수요가 감소하는 특징을 가지고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
