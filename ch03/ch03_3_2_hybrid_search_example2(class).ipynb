{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import NodeWithScore\n",
    "class HybridSearch():\n",
    "    def __init__(self, bm25_retriever, semantic_retriever,\n",
    "                 bm25_weight=0.5, semantic_weight=0.5):\n",
    "        self.bm25_retriever = bm25_retriever # bm25 검색기 객체\n",
    "        self.semantic_retriever = semantic_retriever # 밀집 검색 객체\n",
    "        self.bm25_weight = bm25_weight # bm25 가중치\n",
    "        self.semantic_weight = semantic_weight # 밀집 검색 가중치\n",
    "\n",
    "    def _get_bm25_retrieve(self, query:str)->list:\n",
    "        result = self.bm25_retriever.retrieve(query)\n",
    "        # (node 객체, 가중치 반영 점수)의 튜플 리스트로 반환\n",
    "        score = [(node.node, node.score * self.bm25_weight) for node in result]\n",
    "        return score\n",
    "\n",
    "    def _get_semantic_retrieve(self, query:str)->list:\n",
    "        result = self.semantic_retriever.retrieve(query)\n",
    "        # (node 객체, 가중치 반영 점수)의 튜플 리스트로 반환\n",
    "        score = [(node.node, node.score * self.semantic_weight) for node in result]\n",
    "        return score\n",
    "\n",
    "    # 혼합 검색 node 반환\n",
    "    def retrieve(self, query:str)->list[NodeWithScore]:\n",
    "        bm25_result = self._get_bm25_retrieve(query=query)\n",
    "        semantic_result = self._get_semantic_retrieve(query=query)\n",
    "        \n",
    "        # key: node 객체, value: 최종 점수\n",
    "        combined_scores = {}\n",
    "\n",
    "        # BM25 점수 반영\n",
    "        for node, score in bm25_result:\n",
    "            combined_scores[node.text] = {'node':node, 'score':score}\n",
    "        \n",
    "        # Semantic 점수 반영\n",
    "        for node, score in semantic_result:\n",
    "            if node.text in combined_scores: # 이미 존재하는 청크인 경우\n",
    "                combined_scores[node.text]['score'] += score\n",
    "            else: # 새로운 청크인 경우\n",
    "                \n",
    "                combined_scores[node.text] = {'node':node, 'score':score}\n",
    "        \n",
    "        # NodeWithScore 형태 리스트로 변환\n",
    "        final_results = []\n",
    "        for _, node_info in combined_scores.items():\n",
    "            if node_info['score'] == 0: continue # 점수가 0점인 경우 제외\n",
    "            final_results.append(NodeWithScore(node=node_info['node'], score=node_info['score']))\n",
    "        return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step1. 문서 준비\n",
    "sample_documents = [\n",
    "    \"RAG 기술의 최신 동향\",\n",
    "    \"최신 검색 증강 시스템 연구\",\n",
    "    \"RAG을 활용한 챗봇 개발\",\n",
    "    \"RAG와 Dense Passage Retrieval의 비교 연구\",\n",
    "    \"최신 AI 논문: 검색 증강 기술\",\n",
    "]\n",
    "\n",
    "### Document 객체 생성\n",
    "from llama_index.core import Document\n",
    "documents = [Document(text=doc) for doc in sample_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step2. 희소 검색을 위한 형태소 분석기 함수 작성\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt() # Okt 형태소 분석기 초기화\n",
    "def tokenize_korean_text(text): # 문서 내용 토큰화 함수 정의\n",
    "    return okt.morphs(text)  # 한국어 형태소 기반 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Step3-1 API KEY 설정\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step3-2.  LLM 및 Embedding Model 설정\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-4o\", temperature=0.5)  # 모델명은 예시\n",
    "embedding_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step4. 청킹을 위한 Splitter 설정\n",
    "# 청킹 (chunking)\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./index/ch03_hybrid_search_keyword_storage\\docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./index/ch03_hybrid_search_keyword_storage\\index_store.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./index/ch03_hybrid_search_vertor_storage\\docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./index/ch03_hybrid_search_vertor_storage\\index_store.json.\n"
     ]
    }
   ],
   "source": [
    "### 저장 된 인덱스 불러오기\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "storage_context1 = StorageContext.from_defaults(persist_dir=\"./index/ch03_hybrid_search_keyword_storage\") \n",
    "storage_context2 = StorageContext.from_defaults(persist_dir=\"./index/ch03_hybrid_search_vertor_storage\") \n",
    "keyword_index = load_index_from_storage(storage_context1) # BM25 검색 인덱스 로드\n",
    "vector_index = load_index_from_storage(storage_context2) # 밀집 검색 인덱스 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resource module not available on Windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\OneDrive\\Desktop\\Project\\llamaindex_practice\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## bm25 retriever 객체 생성(keyword_index 연결)\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    index=keyword_index,\n",
    "    similarity_top_k=5,\n",
    ")\n",
    "\n",
    "# dense retriever 객체 생성(vector_index 연결)\n",
    "semantic_retriever = vector_index.as_retriever(similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='검색 증강 생성'\n",
    "# 혼합 검색 클래스 객체 생성\n",
    "hybrid = HybridSearch(bm25_retriever=bm25_retriever,\n",
    "                      semantic_retriever=semantic_retriever)\n",
    "combined_score = hybrid.retrieve(query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: cf3078fc-e094-453f-90a0-098c0e1b154d\n",
      "Text: 최신 검색 증강 시스템 연구\n",
      "Score:  0.802\n",
      "\n",
      "Node ID: 6eb47b4d-ada6-4b41-a6e4-d04efc9728af\n",
      "Text: 최신 AI 논문: 검색 증강 기술\n",
      "Score:  0.759\n",
      "\n",
      "Node ID: 5f9fd9a7-b540-4a98-9a6a-7bf1df2a0d53\n",
      "Text: RAG와 Dense Passage Retrieval의 비교 연구\n",
      "Score:  0.396\n",
      "\n",
      "Node ID: e15b2740-b07b-4ad6-9299-b0bd89791aac\n",
      "Text: RAG을 활용한 챗봇 개발\n",
      "Score:  0.410\n",
      "\n",
      "Node ID: 49281b38-004a-4dcf-8b6e-9ea04fff9765\n",
      "Text: RAG 기술의 최신 동향\n",
      "Score:  0.403\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rufrhk 결과 출력\n",
    "for node_score_obj in combined_score:\n",
    "    print(node_score_obj)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
