{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. PDF 문서 이미지 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install unstructured==0.12.5 unstructured-inference\n",
    "%pip install pdfminer.six==20231228\n",
    "%pip install pdf2image pillow-heif opencv-python\n",
    "%pip install llama-index-vector-stores-qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 문서 추출\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "image_save_path = \"./data_image/pdf2image\" # 추출 할 이미지 저장 경로\n",
    "fpath = \"./data/2005.11401v4.pdf\" # 사용 할 논문 파일 경로\n",
    "'''\n",
    "unstructured를 사용하여 pdf 이미지만 파일로 추출하기\n",
    "'''\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=fpath,\n",
    "    extract_images_in_pdf=True, # 이미지 추출\n",
    "    extract_image_block_output_dir=image_save_path,\n",
    "    strategy=\"fast\",              # pdfminer / 기본 파서만 사용\n",
    "    infer_table_structure=False   # 표 구조 분석 모델 호출 차단\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지를 출력하는 함수 작성\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_images(image_paths):\n",
    "    images_shown = 0\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    for img_path in image_paths:\n",
    "        img_path = img_path.lower() # 소문자화\n",
    "        if \".jpg\" in img_path:\n",
    "            image = Image.open(img_path)\n",
    "\n",
    "            plt.subplot(3, 3, images_shown + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            images_shown += 1\n",
    "            if images_shown >= 9:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추출 된 이미지 출력해보기\n",
    "import os\n",
    "image_save_path = \"./data_image/pdf2image\" # 추출 할 이미지 저장 경로\n",
    "file_list = [os.path.join(image_save_path, f) for f in os.listdir(image_save_path)]\n",
    "plot_images(file_list) # 추출 된 이미지 plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PDF 문서 테이블, 텍스트 등 파서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지를 문서로 읽어오기\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "image_save_path = \"./data_image/pdf2image\" # 이미지가 저장 된 경로\n",
    "image_documents = SimpleDirectoryReader(input_dir=image_save_path).load_data()\n",
    "print(f\"이미지 문서 개수 : {len(image_documents)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장 된 이미지 타입 확인\n",
    "for doc in image_documents:\n",
    "    print(type(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1. .evn 파일 로드 및 비동기 문제 해결\n",
    "from dotenv import load_dotenv \n",
    "import nest_asyncio\n",
    "load_dotenv() # 환경 변수 설정\n",
    "nest_asyncio.apply() # 비동기 처리 문제 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2. 라마파스 사용하여 문서 로드하기\n",
    "from llama_parse import LlamaParse\n",
    "parser = LlamaParse(result_type=\"text\") # markdown 혹은 text 사용 가능\n",
    "file_extractor = {\".pdf\": parser}  # PDF 파일을 LlamaParse로 처리하도록 설정\n",
    "fpath = \"./data/2005.11401v4.pdf\" # 사용 할 논문 파일 경로\n",
    "text_documents = SimpleDirectoryReader(\n",
    "    input_files=[fpath], # 논문 저장경로\n",
    "    file_extractor=file_extractor\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"논문 TEXT 문서 개수 : {len(text_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 이미지 캡셔닝\n",
    "\n",
    "ImageDocuments는 llama-index 0.14.2(25년9월 기준) 버전 기준으로 멀티모달 LLM 호출에 전달할 때, 사용불가\n",
    "ImageNode로 사용하는 것으로 권장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1. ImageDocument를 ImageNode로 변환\n",
    "\n",
    "from llama_index.core.schema import ImageDocument, ImageNode \n",
    "from typing import List\n",
    "\n",
    "# 이미지 문서 -> 이미지 노드 변환함수 작성(이미지 경로 포함)\n",
    "def docs_to_image_nodes(img_docs: List[ImageDocument]) -> List[ImageNode]:\n",
    "    nodes = []\n",
    "    for d in img_docs:\n",
    "        # 우선순위: d.image_path -> d.metadata[\"file_path\"] -> None\n",
    "        path = getattr(d, \"image_path\", None) or d.metadata.get(\"file_path\")\n",
    "        node = ImageNode(\n",
    "            image_path=path,                # ✅ 경로를 여기에\n",
    "            image_url=getattr(d, \"image_url\", None),\n",
    "            image=getattr(d, \"image\", None),  # base64일 수 있음\n",
    "            metadata=dict(d.metadata),     # 기존 메타데이터 유지\n",
    "            text=getattr(d, \"text\", None),\n",
    "        )\n",
    "        nodes.append(node)\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2. 이미지 요약을 생성하고 캡션을 추가하는 함수 작성\n",
    "\n",
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "\n",
    "# 이미지 캡셔닝 함수 정의\n",
    "def summarize_images_captioning(image_nodes: list[ImageNode]) -> List[ImageNode]:\n",
    "    # 멀티모달 모델 인스턴스화\n",
    "    openai_mm_llm = OpenAIMultiModal(\n",
    "        model='gpt-4o',         # 실제 사용하는 모델명으로 변경\n",
    "        max_new_tokens=3000,\n",
    "        temperature=0.5\n",
    "    )\n",
    "    \n",
    "    prompt_template = \"\"\"\n",
    "    당신은 주어진 이미지를 텍스트로 설명을 바꾸는 역할을 해야 합니다.\n",
    "    이미지에 대한 상세 설명을 해주세요.\n",
    "    해당 이미지에 텍스트나 숫자, 수식 등이 있는 경우 해당 용어를 반드시 모두 사용해서 어떤 그림인지 설명해주세요.\n",
    "    그래프나 차트의 경우도 숫자, 수식 등의 데이터를 포함하여 상세 설명해주시길 바랍니다.\n",
    "    \"\"\"\n",
    "    \n",
    "    for img_node in image_nodes:\n",
    "        # 이미지를 단건으로 넣어서 각각 요약(캡션) 생성\n",
    "        response = openai_mm_llm.complete(\n",
    "            prompt=prompt_template,\n",
    "            image_documents=[img_node]\n",
    "        )\n",
    "        # ImageNode의 metadata에 요약 text를 저장하는('caption')을 추가\n",
    "        img_node.metadata[\"caption\"] = response.text\n",
    "    \n",
    "    # 메타데이터가 갱신된 ImageDocument 리스트 반환\n",
    "    return image_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 문서를 노드로 변환시키고 LLM을 통한 설명 요약을 캡션에 추가 \n",
    "doc_to_nodes = docs_to_image_nodes(img_docs=image_documents) # 이미지 노드 변환\n",
    "image_nodes = summarize_images_captioning(image_nodes=doc_to_nodes) # 이미지 노드에 캡셔닝 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 노드 첫 요소 출력\n",
    "print(f'Type : {type(image_nodes[0])}\\n')\n",
    "print('Metadata :')\n",
    "print(image_nodes[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3. ImageNode를 Document로 변환\n",
    "\n",
    "'''\n",
    "이미지 요약 텍스트를 추가한 Image Node를 Document 객체로 변환한다.\n",
    "실제 이미지를 텍스트를 통해 검색될 수 있도록 객체를 변환하는 과정\n",
    "이 때 image id를 추가해 원본 이미지를 참조할 수 있게 한다.\n",
    "'''\n",
    "from llama_index.core.schema import Document, NodeRelationship, RelatedNodeInfo\n",
    "new_caption_docs = []\n",
    "for img_node in image_nodes:\n",
    "    caption = img_node.metadata[\"caption\"]  # 앞 단계에서 넣어둔 캡션\n",
    "    caption_doc = Document(\n",
    "        text=caption,\n",
    "        metadata={\n",
    "            \"source\": img_node.metadata.get(\"source\", \"\"),\n",
    "            \"is_image_caption\": True, # 이미지 캡셔닝 대상임을 표시\n",
    "            \n",
    "            # 조회 편의용으로 저장\n",
    "            \"image_path\": img_node.image_path,      # node의 image_path\n",
    "            \"image_node_id\": img_node.id_,          # 조회 키\n",
    "        },\n",
    "    )\n",
    "    # 관계로도 연결\n",
    "    caption_doc.relationships[NodeRelationship.SOURCE] = RelatedNodeInfo(\n",
    "        node_id=img_node.id_\n",
    "    )\n",
    "    new_caption_docs.append(caption_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_caption_docs[0]  # 제대로 변환되었는지 첫 요소 확인하기(ImageNode -> Document로 제대로 변환되었는지 확인)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 논문 text document + 이미지 text 캡셔닝 Document + 이미지 Document\n",
    "documents = text_documents+new_caption_docs+image_documents \n",
    "print(f'문서의 개수 : {len(documents)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 인덱스 생성 및 RAG 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1. 인덱스 생성 및 저장\n",
    "# 신규 생성 (기존 경로가 이미 존재 할 경우 불러옴)\n",
    "from qdrant_client import QdrantClient\n",
    "client = QdrantClient(path=\"./index/ch05_3_4_pdf_to_index_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "# 텍스트에 대한 벡터 스토어 구성\n",
    "text_store= QdrantVectorStore(client=client,\n",
    "                              collection_name=\"text_collection\")\n",
    "# 이미지에 대한 벡터 스토어 구성\n",
    "image_store = QdrantVectorStore(client=client, \n",
    "                              collection_name=\"image_collection\")\n",
    "\n",
    "# stroage context를 통해 멀티모달 벡터 스토어 및 인덱스를 통합관리\n",
    "from llama_index.core import StorageContext\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=text_store,\n",
    "    image_store=image_store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멀티 모달 인덱스 구성 및 저장\n",
    "from llama_index.core.indices.multi_modal.base import MultiModalVectorStoreIndex\n",
    "index = MultiModalVectorStoreIndex.from_documents(\n",
    "    documents=documents, # 표/텍스트 문서+이미지 문서+이미지 캡셔닝 문서\n",
    "    storage_context=storage_context,\n",
    ")\n",
    "index.storage_context.persist(persist_dir=\"./index/ch05_3_4_pdf_to_index_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 StorageContext 로드\n",
    "from llama_index.core import load_index_from_storage\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=\"./index/ch05_3_4_pdf_to_index_db\",\n",
    "    vector_store=text_store,\n",
    "    image_store=image_store\n",
    ")\n",
    "\n",
    "# 저장된 인덱스 로드\n",
    "index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 유사도 검색 및 결과 출력(이미지 포함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2. 유사도 검색 및 결과 출력(이미지 포함)\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 이미지를 출력하는 함수 작성\n",
    "def plot_images(image_paths):\n",
    "    images_shown = 0\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    for img_path in image_paths:\n",
    "        img_path = img_path.lower() # 소문자화\n",
    "        if \".jpg\" in img_path:\n",
    "            image = Image.open(img_path)\n",
    "\n",
    "            plt.subplot(3, 3, images_shown + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "            images_shown += 1\n",
    "            if images_shown >= 9:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_engine = index.as_retriever(\n",
    "    similarity_top_k = 2, # 텍스트(Document) 노드 상위 2개 검색\n",
    "    image_similarity_top_k = 2, # 이미지(ImageDocument) 노드 상위 2개 검색\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쿼리 입력 및 검색(이미지, 텍스트 조회)\n",
    "nodes = retriever_engine.retrieve('Please tell me about Query Encoder') \n",
    "\n",
    "# 텍스트 요약 캡셔닝을 한 문서가 제대로 검색 출력되는지 확인\n",
    "captioning_image_paths=[] # Document 중 이미지 캡셔닝 문서 Node\n",
    "image_node_paths=[] # ImageDocument로 검색 된 Node\n",
    "for node in nodes:\n",
    "    print(node)\n",
    "    if 'is_image_caption' in node.metadata: # 노드의 메타데이터에 is_image_caption 키가 있는 경우  \n",
    "        captioning_image_paths.append(node.metadata['image_path'])\n",
    "    if type(node.node) == ImageNode: # ImageDocument로 인해 생성 된 노드인 경우\n",
    "        image_node_paths.append(node.metadata['file_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('** 이미지 캡셔닝 문서로 검색 된 이미지 출력 *')\n",
    "plot_images(image_paths=captioning_image_paths) # 이미지 plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('** ImageDocument로 검색 된 노드 이미지 출력 *')\n",
    "plot_images(image_paths=image_node_paths) # 이미지 plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 노드의 타입 및 메타데이터 확인\n",
    "for n in nodes:\n",
    "    node = getattr(n, \"node\", n)   # NodeWithScore → node 꺼내기\n",
    "    print(f\"Type : {type(node)}\")\n",
    "    print(f\"Metadata keys : {node.metadata.keys()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 쿼리 엔진 검색 시스템 구현\n",
    "[참고] 이미지 캡셔닝을 통해 모든 imageDocument -> Document로 변경 후, Document로만 RAG 시스템을 구성시 OpenAIMultiModal 클래스를 사용하지 않고 \n",
    "기존 일반 텍스트 처리 모델 OpenAI 를 사용해도 무관합니다. Document 객체에서 이미지에 대한 metadata 를 추가했기 때문입니다.\n",
    "단, 이미지 문서인 ImageDocument를 같이 입력하는 멀티 모달 RAG를 같이 구현하는 경우에는 OpenAIMultiModal을 사용해야 합니다. 이번 실습에서는 ImageDocument와 Document가 섞여있는 구조로 OpenAIMultiModel을 사용합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3. 쿼리 엔진 구현\n",
    "from llama_index.multi_modal_llms.openai import OpenAIMultiModal\n",
    "openai_mm_llm = OpenAIMultiModal(model=\"gpt-4o\", \n",
    "                                 max_new_tokens=3000,)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=openai_mm_llm,\n",
    "    similarity_top_k = 1, # Document 1개 반환\n",
    "    image_similarity_top_k = 1 # ImageDocument 1개 반환\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "# 프롬프트 템플릿 정의\n",
    "template = (\n",
    "    \"제공 된 정보를 참고하여 한국어로 답변을 해주세요.\\n\"\n",
    "    \"{query_str}\\n\"\n",
    ")\n",
    "prompt_template = PromptTemplate(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"Please tell me about Query Encoder\"\n",
    "final_prompt = prompt_template.format(query_str=query_str)\n",
    "response = query_engine.query(final_prompt)\n",
    "\n",
    "# prompt와 response 출력\n",
    "print(f\"Prompt:{final_prompt}\")\n",
    "print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import ImageNode, TextNode\n",
    "image_paths=[] # 이미지 경로 및 노드 타입 저장\n",
    "for n in response.source_nodes:\n",
    "    node = getattr(n, \"node\", n)   # NodeWithScore → node 꺼내기\n",
    "    print(f\"Node : \\n{node}\\nType : {type(node)}\\nMeta data : \\n{node.metadata}\\n\")\n",
    "    if isinstance(node, TextNode):\n",
    "        if 'is_image_caption' in node.metadata.keys():\n",
    "            image_paths.append(('Text Node', node.metadata['image_path']))\n",
    "    if isinstance(node, ImageNode):\n",
    "        image_paths.append(('Image Node', node.metadata['file_path']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 응답에 참고한 이미지 패스\n",
    "for path in image_paths:\n",
    "    print(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
